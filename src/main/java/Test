import json
import re
import pandas as pd
from transformers import pipeline
import PyPDF2  # For PDF text extraction

# Step 1: Extract Text from PDF
def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF file.
    """
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    return text

# Step 2: Extract Rules from Unstructured Text
def extract_rules_from_text(document_text):
    """
    Use a Generative AI model to extract rules from the regulatory document.
    """
    generator = pipeline("text-generation", model="gpt2", device="cpu")  # Change to "cuda" or "mps" if GPU is available

    prompt = f"""
    Analyze the following regulatory document and extract data profiling rules:
    {document_text}

    Rules:
    """
    response = generator(prompt, max_new_tokens=200, truncation=True)
    rules_text = response[0]["generated_text"].strip()

    # Parse the rules into a structured format
    rules = []
    for line in rules_text.split("\n"):
        if "Field" in line:
            field_name = re.search(r"Field '(.*?)'", line)
            rule_description = line.strip()
            if field_name:
                rules.append({
                    "field": field_name.group(1),
                    "rule": rule_description
                })
    return rules

# Step 3: Map Rules to Data Fields
def map_rules_to_fields(rules, df_columns):
    """
    Map the extracted rules to the columns in the dataset.
    """
    field_mapping = {}
    for rule in rules:
        field = rule["field"]
        if field in df_columns:
            field_mapping[field] = rule["rule"]
    return field_mapping

# Step 4: Validate Data Against Rules
def validate_data(df, field_mapping):
    """
    Validate the data against the mapped rules.
    """
    validation_results = {}
    for field, rule in field_mapping.items():
        if field in df.columns:
            if "must be a 10-digit number" in rule:
                validation_results[field] = df[field].apply(lambda x: len(str(x)) == 10 and str(x).isdigit())
            elif "must be a positive number" in rule:
                validation_results[field] = df[field].apply(lambda x: x > 0)
            elif "must be in YYYY-MM-DD format" in rule:
                validation_results[field] = df[field].apply(lambda x: bool(re.match(r"^\d{4}-\d{2}-\d{2}$", str(x))))
            elif "must be above" in rule:
                threshold = float(re.search(r"above (.*?)%", rule).group(1))
                validation_results[field] = df[field].apply(lambda x: x > threshold)
    return validation_results

# Step 5: Suggest Remediation Actions
def suggest_remediation(df, field_mapping, validation_results):
    """
    Suggest remediation actions for each violation.
    """
    suggestions = []
    generator = pipeline("text-generation", model="gpt2", device="cpu")  # Change to "cuda" or "mps" if GPU is available

    for index, row in df.iterrows():
        # Initialize a dictionary for the current transaction
        transaction = {
            "transaction_row": row.to_dict(),
            "violations": []
        }

        # Check each field for violations
        for field, rule in field_mapping.items():
            if field in validation_results and not validation_results[field][index]:
                prompt = f"""
                The following transaction row has been flagged as anomalous:
                {row.to_dict()}

                The rule violated is:
                {rule}

                The field '{field}' failed validation. Suggest a remediation action based on the above rule.

                Remediation Action:
                """
                # Generate text with max_new_tokens instead of max_length
                response = generator(prompt, max_new_tokens=50, truncation=True)
                remediation_action = response[0]["generated_text"].strip()

                # Clean up the remediation action
                remediation_action = cleanup_remediation_action(remediation_action, field, row[field])

                # Add the violation details to the transaction
                transaction["violations"].append({
                    "violated_field": field,
                    "violated_rule": rule,
                    "remediation_action": remediation_action
                })

        # Add the transaction to the suggestions list if there are violations
        if transaction["violations"]:
            suggestions.append(transaction)

    # Convert the list of suggestions to a JSON-formatted string
    return json.dumps(suggestions, indent=4)

def cleanup_remediation_action(action, field, value):
    """
    Clean up the remediation action to make it concise and consistent.
    """
    # Remove unnecessary phrases like "Remediation Action:"
    if "Remediation Action:" in action:
        action = action.split("Remediation Action:")[-1].strip()

    # Remove repeated phrases or redundant text
    action = action.replace("The following transaction row has been flagged as anomalous:", "").strip()
    action = action.replace("The rule violated is:", "").strip()
    action = action.replace("The field failed validation.", "").strip()

    # Ensure the action is concise and relevant
    if not action.startswith("Please"):
        action = f"Please correct the value of '{field}' from '{value}' to comply with the rule."

    return action

# Main Workflow
if __name__ == "__main__":
    # Step 1: Extract text from the regulatory PDF
    pdf_path = "regulatory_document.pdf"  # Replace with your PDF file path
    regulatory_text = extract_text_from_pdf(pdf_path)
    print("Extracted Text from PDF:\n", regulatory_text)

    # Step 2: Extract rules from the regulatory text
    rules = extract_rules_from_text(regulatory_text)
    print("\nExtracted Rules:\n", rules)

    # Step 3: Map rules to dataset fields
    # Load sample dataset from CSV
    csv_path = "sample_data.csv"  # Replace with your CSV file path
    df = pd.read_csv(csv_path)
    field_mapping = map_rules_to_fields(rules, df.columns)
    print("\nField Mapping:\n", field_mapping)

    # Step 4: Validate data against mapped rules
    validation_results = validate_data(df, field_mapping)
    print("\nValidation Results:\n", validation_results)

    # Step 5: Suggest remediation actions for flagged issues
    remediation_actions = suggest_remediation(df, field_mapping, validation_results)
    print("\nSuggested Remediation Actions (JSON):\n", remediation_actions)

    # Save JSON output to a file
    with open("remediation_suggestions.json", "w") as file:
        file.write(remediation_actions)
